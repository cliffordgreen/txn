{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Hybrid Transaction Model Training\n",
    "\n",
    "This notebook demonstrates how to train the EnhancedHybridTransactionModel on a p3.2xlarge AWS instance with V100 GPU acceleration, processing multiple parquet files (~100) efficiently.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "The EnhancedHybridTransactionModel combines:\n",
    "1. Graph-based transaction relationships (company, merchant, industry, price)\n",
    "2. Temporal patterns with company-based grouping\n",
    "3. Hyperbolic encoding for hierarchical relationships\n",
    "\n",
    "## Optimization Features\n",
    "\n",
    "- Mixed precision training (FP16)\n",
    "- GPU memory optimization\n",
    "- Parallel data loading\n",
    "- Batched processing for large datasets\n",
    "- CUDA graph optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.2.4-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-2.2.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(f\"Added module path: {module_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Memory Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset for parquet files\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ParquetTransactionDataset(Dataset):\n",
    "    def __init__(self, parquet_files, preprocess_fn=None, transform_fn=None):\n",
    "        self.parquet_files = parquet_files\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.transform_fn = transform_fn\n",
    "        \n",
    "        # Get total number of rows across all files\n",
    "        self.file_row_counts = []\n",
    "        self.total_rows = 0\n",
    "        \n",
    "        for file in tqdm(parquet_files, desc=\"Counting rows\"):\n",
    "            # For very large files, just read metadata or count rows efficiently\n",
    "            try:\n",
    "                # Try pyarrow method to get row count\n",
    "                import pyarrow.parquet as pq\n",
    "                row_count = pq.read_metadata(file).num_rows\n",
    "            except:\n",
    "                try:\n",
    "                    # Fall back to pandas with just column names\n",
    "                    row_count = pd.read_parquet(file, columns=['txn_id']).shape[0]\n",
    "                except:\n",
    "                    # Last resort: skip this file\n",
    "                    print(f\"Warning: Could not determine row count for {file}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "            self.file_row_counts.append(row_count)\n",
    "            self.total_rows += row_count\n",
    "            \n",
    "        # Create lookup table for file index and row index\n",
    "        self.lookup = []\n",
    "        for file_idx, row_count in enumerate(self.file_row_counts):\n",
    "            self.lookup.extend([(file_idx, row_idx) for row_idx in range(row_count)])\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(parquet_files)} files and {self.total_rows} total rows\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_rows\n",
    "    \n",
    "    def _read_row_from_file(self, file_path, row_idx):\n",
    "        \"\"\"Read a specific row from a file without loading the entire file\"\"\"\n",
    "        try:\n",
    "            # First try to use fast row access\n",
    "            table = pd.read_parquet(file_path, engine='pyarrow')\n",
    "            row = table.iloc[row_idx:row_idx+1]\n",
    "            \n",
    "            if row.empty:\n",
    "                raise ValueError(f\"Row {row_idx} not found in {file_path}\")\n",
    "                \n",
    "            return row\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading row {row_idx} from {file_path}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.lookup):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset with {len(self.lookup)} items\")\n",
    "            \n",
    "        file_idx, row_idx = self.lookup[idx]\n",
    "        file_path = self.parquet_files[file_idx]\n",
    "        \n",
    "        df = self._read_row_from_file(file_path, row_idx)\n",
    "        \n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if self.preprocess_fn:\n",
    "            df = self.preprocess_fn(df)\n",
    "            \n",
    "        if self.transform_fn:\n",
    "            return self.transform_fn(df)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def get_sample_batch(self, sample_size=100):\n",
    "        \"\"\"Get a small sample batch for metadata and model initialization\"\"\"\n",
    "        print(f\"Getting sample batch of {sample_size} rows...\")\n",
    "        \n",
    "        # Use a small sample size to avoid memory issues\n",
    "        sample_size = min(sample_size, self.total_rows)\n",
    "        \n",
    "        # Sample evenly across files\n",
    "        rows_per_file = max(1, sample_size // len(self.parquet_files))\n",
    "        \n",
    "        sample_dfs = []\n",
    "        \n",
    "        for file_idx, file_path in enumerate(self.parquet_files):\n",
    "            if file_idx >= len(self.file_row_counts):\n",
    "                continue\n",
    "                \n",
    "            row_count = self.file_row_counts[file_idx]\n",
    "            if row_count == 0:\n",
    "                continue\n",
    "                \n",
    "            # Sample rows from this file\n",
    "            try:\n",
    "                # Try to read just a few rows from the start of the file\n",
    "                sample_rows = min(rows_per_file, row_count)\n",
    "                file_df = pd.read_parquet(file_path, engine='pyarrow').head(sample_rows)\n",
    "                \n",
    "                if self.preprocess_fn:\n",
    "                    file_df = self.preprocess_fn(file_df)\n",
    "                    \n",
    "                sample_dfs.append(file_df)\n",
    "                \n",
    "                # Stop if we have enough samples\n",
    "                if sum(len(df) for df in sample_dfs) >= sample_size:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error sampling from {file_path}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if not sample_dfs:\n",
    "            raise ValueError(\"Could not get any sample data from the dataset\")\n",
    "            \n",
    "        return pd.concat(sample_dfs, ignore_index=True)\n",
    "        \n",
    "    def get_batch_df(self, indices):\n",
    "        \"\"\"Get a batch of rows as a single DataFrame\"\"\"\n",
    "        print(f\"Warning: get_batch_df called with {len(indices)} indices. This may be slow and memory-intensive.\")\n",
    "        print(f\"Consider using get_sample_batch() for metadata extraction instead.\")\n",
    "        \n",
    "        # For very large batches, sample instead\n",
    "        if len(indices) > 1000:\n",
    "            print(f\"Too many indices ({len(indices)}), sampling 1000 indices instead\")\n",
    "            import random\n",
    "            indices = random.sample(indices, 1000)\n",
    "        \n",
    "        dfs = []\n",
    "        for idx in tqdm(indices, desc=\"Loading batch rows\"):\n",
    "            try:\n",
    "                df = self.__getitem__(idx)\n",
    "                if not df.empty:\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting item {idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            raise ValueError(\"All dataframes in batch were empty\")\n",
    "            \n",
    "        return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "class Config:\n",
    "    # Data configuration\n",
    "    data_dir = \"/path/to/parquet/files\"  # Replace with your actual path\n",
    "    output_dir = \"../models/enhanced_model_output\"\n",
    "    batch_size = 64\n",
    "    num_workers = 4\n",
    "    prefetch_factor = 2\n",
    "    max_files = 100  # Max number of parquet files to process\n",
    "    \n",
    "    # Model configuration\n",
    "    hidden_dim = 256\n",
    "    num_heads = 8\n",
    "    num_graph_layers = 2\n",
    "    num_temporal_layers = 2\n",
    "    dropout = 0.2\n",
    "    use_hyperbolic = True\n",
    "    use_neural_ode = False  # Set to False for faster training\n",
    "    use_text = False  # Set to True if transaction descriptions are available\n",
    "    multi_task = True\n",
    "    num_relations = 5  # company, merchant, industry, price, temporal\n",
    "    \n",
    "    # Training configuration\n",
    "    learning_rate = 3e-4\n",
    "    weight_decay = 1e-5\n",
    "    num_epochs = 10\n",
    "    patience = 3\n",
    "    grad_clip = 1.0\n",
    "    \n",
    "    # GPU optimization\n",
    "    use_amp = True  # Use mixed precision training\n",
    "    use_cuda_graphs = True  # Use CUDA graphs for optimization\n",
    "    cuda_graph_batch_size = batch_size  # Fixed batch size for CUDA graphs\n",
    "    \n",
    "    # XGBoost integration\n",
    "    extract_embeddings = True\n",
    "    embedding_output_file = \"transaction_embeddings.pkl\"\n",
    "    \n",
    "    # Metrics\n",
    "    eval_steps = 100\n",
    "    log_steps = 10\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Define functions to load and process parquet files efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel parquet file loading\n",
    "def get_parquet_files(data_dir, max_files=None):\n",
    "    \"\"\"Get list of parquet files from directory\"\"\"\n",
    "    parquet_files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    \n",
    "    if max_files is not None and len(parquet_files) > max_files:\n",
    "        parquet_files = parquet_files[:max_files]\n",
    "        \n",
    "    print(f\"Found {len(parquet_files)} parquet files\")\n",
    "    return parquet_files\n",
    "\n",
    "# Custom PyTorch Dataset for parquet files\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ParquetTransactionDataset(Dataset):\n",
    "    def __init__(self, parquet_files, preprocess_fn=None, transform_fn=None):\n",
    "        self.parquet_files = parquet_files\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.transform_fn = transform_fn\n",
    "        \n",
    "        # Get total number of rows across all files\n",
    "        self.file_row_counts = []\n",
    "        self.total_rows = 0\n",
    "        \n",
    "        for file in tqdm(parquet_files, desc=\"Counting rows\"):\n",
    "            row_count = pd.read_parquet(file, columns=['txn_id']).shape[0]\n",
    "            self.file_row_counts.append(row_count)\n",
    "            self.total_rows += row_count\n",
    "            \n",
    "        # Create lookup table for file index and row index\n",
    "        self.lookup = []\n",
    "        for file_idx, row_count in enumerate(self.file_row_counts):\n",
    "            self.lookup.extend([(file_idx, row_idx) for row_idx in range(row_count)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_rows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.lookup):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset with {len(self.lookup)} items\")\n",
    "            \n",
    "        file_idx, row_idx = self.lookup[idx]\n",
    "        file_path = self.parquet_files[file_idx]\n",
    "        \n",
    "        # Read single row from parquet file\n",
    "        df = pd.read_parquet(file_path, filters=[(\"row_idx\", \"==\", row_idx)])\n",
    "        \n",
    "        if df.empty:\n",
    "            # Fall back to reading the whole file and extracting the row\n",
    "            df = pd.read_parquet(file_path).iloc[row_idx:row_idx+1]\n",
    "        \n",
    "        if self.preprocess_fn:\n",
    "            df = self.preprocess_fn(df)\n",
    "            \n",
    "        if self.transform_fn:\n",
    "            return self.transform_fn(df)\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "    def get_batch_df(self, indices):\n",
    "        \"\"\"Get a batch of rows as a single DataFrame\"\"\"\n",
    "        dfs = []\n",
    "        for idx in indices:\n",
    "            dfs.append(self.__getitem__(idx))\n",
    "        return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient DataFrame preprocessing\n",
    "def preprocess_transactions(df):\n",
    "    \"\"\"Preprocess transaction DataFrame for model input\"\"\"\n",
    "    # Ensure required columns are present\n",
    "    required_columns = ['company_id', 'merchant_id', 'amount', 'category_id']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column {col} not found in dataset\")\n",
    "    \n",
    "    # Optimize memory usage\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    \n",
    "    # Ensure timestamp is in proper format\n",
    "    if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort by timestamp if available\n",
    "    if 'timestamp' in df.columns:\n",
    "        df = df.sort_values('timestamp')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient batch processing function\n",
    "def prepare_model_inputs(batch_df, model):\n",
    "    \"\"\"Prepare model inputs from a batch DataFrame\"\"\"\n",
    "    # Use the model's data preparation function\n",
    "    data = model.prepare_data_from_dataframe(batch_df)\n",
    "    \n",
    "    # Move tensors to the correct device\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            data[key] = value.to(device)\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = {\n",
    "        'category': torch.tensor(batch_df['category_id'].values, dtype=torch.long).to(device)\n",
    "    }\n",
    "    \n",
    "    if 'tax_type_id' in batch_df.columns:\n",
    "        labels['tax_type'] = torch.tensor(batch_df['tax_type_id'].values, dtype=torch.long).to(device)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Graph Optimization\n",
    "\n",
    "For p3.2xlarge instances with V100 GPUs, we can use CUDA graph optimization to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Graph optimization functions\n",
    "def create_cuda_graph(model, sample_data):\n",
    "    \"\"\"Create CUDA graph for model inference\"\"\"\n",
    "    if not torch.cuda.is_available() or not config.use_cuda_graphs:\n",
    "        return None\n",
    "    \n",
    "    print(\"Creating CUDA graph for optimized inference...\")\n",
    "    \n",
    "    # Extract required inputs from sample data\n",
    "    inputs = {\n",
    "        'x': sample_data['x'],\n",
    "        'edge_index': sample_data['edge_index'],\n",
    "        'edge_type': sample_data['edge_type'],\n",
    "        'edge_attr': sample_data['edge_attr'],\n",
    "        'seq_features': sample_data['seq_features'],\n",
    "        'timestamps': sample_data['timestamps'],\n",
    "        'tabular_features': sample_data['tabular_features'],\n",
    "        't0': sample_data['t0'],\n",
    "        't1': sample_data['t1'],\n",
    "        'company_features': sample_data['company_features'],\n",
    "        'company_ids': sample_data['company_ids'],\n",
    "        'batch_size': sample_data['batch_size'],\n",
    "        'seq_len': sample_data['seq_len']\n",
    "    }\n",
    "    \n",
    "    # Ensure all inputs are on the correct device\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(device)\n",
    "    \n",
    "    # Create static input batch\n",
    "    static_inputs = {}\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            static_inputs[k] = v.clone()\n",
    "        else:\n",
    "            static_inputs[k] = v\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(3):\n",
    "        with torch.no_grad():\n",
    "            model(**static_inputs)\n",
    "    \n",
    "    # Capture graph\n",
    "    g = torch.cuda.CUDAGraph()\n",
    "    with torch.cuda.graph(g):\n",
    "        static_outputs = model(**static_inputs)\n",
    "    \n",
    "    return {\n",
    "        'graph': g,\n",
    "        'static_inputs': static_inputs,\n",
    "        'static_outputs': static_outputs\n",
    "    }\n",
    "\n",
    "def run_with_cuda_graph(cuda_graph, data):\n",
    "    \"\"\"Run inference using CUDA graph\"\"\"\n",
    "    # Update static inputs with new data\n",
    "    for k, v in data.items():\n",
    "        if k in cuda_graph['static_inputs'] and isinstance(v, torch.Tensor):\n",
    "            cuda_graph['static_inputs'][k].copy_(v)\n",
    "    \n",
    "    # Run the graph\n",
    "    cuda_graph['graph'].replay()\n",
    "    \n",
    "    # Return outputs\n",
    "    return cuda_graph['static_outputs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "def initialize_model(hidden_dim, num_categories, num_tax_types):\n",
    "    \"\"\"Initialize the EnhancedHybridTransactionModel\"\"\"\n",
    "    model = EnhancedHybridTransactionModel(\n",
    "        input_dim=hidden_dim,  # We'll project inputs to match this dimension\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=num_categories,\n",
    "        num_heads=config.num_heads,\n",
    "        num_graph_layers=config.num_graph_layers,\n",
    "        num_temporal_layers=config.num_temporal_layers,\n",
    "        dropout=config.dropout,\n",
    "        use_hyperbolic=config.use_hyperbolic,\n",
    "        use_neural_ode=config.use_neural_ode,\n",
    "        use_text=config.use_text,\n",
    "        multi_task=config.multi_task,\n",
    "        tax_type_dim=num_tax_types,\n",
    "        num_relations=config.num_relations,\n",
    "        graph_weight=0.6,\n",
    "        temporal_weight=0.4,\n",
    "        use_dynamic_weighting=True\n",
    "    ).to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function\n",
    "def train(model, train_dataset, val_dataset, config):\n",
    "    \"\"\"Train the model with the given datasets\"\"\"\n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=config.patience // 2,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Mixed precision\n",
    "    scaler = GradScaler() if config.use_amp else None\n",
    "    \n",
    "    # Create train dataloader with random sampling\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        prefetch_factor=config.prefetch_factor,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create validation dataloader\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        prefetch_factor=config.prefetch_factor,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize metrics tracking\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initial CUDA graph (will be updated later)\n",
    "    cuda_graph = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        samples_processed = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        \n",
    "        for step, batch_indices in pbar:\n",
    "            # Get batch dataframe\n",
    "            batch_df = train_dataset.get_batch_df(batch_indices)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            data, labels = prepare_model_inputs(batch_df, model)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if config.use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(\n",
    "                        x=data['x'],\n",
    "                        edge_index=data['edge_index'],\n",
    "                        edge_type=data['edge_type'],\n",
    "                        edge_attr=data['edge_attr'],\n",
    "                        seq_features=data['seq_features'],\n",
    "                        timestamps=data['timestamps'],\n",
    "                        tabular_features=data['tabular_features'],\n",
    "                        t0=data['t0'],\n",
    "                        t1=data['t1'],\n",
    "                        company_features=data['company_features'],\n",
    "                        company_ids=data['company_ids'],\n",
    "                        batch_size=data['batch_size'],\n",
    "                        seq_len=data['seq_len']\n",
    "                    )\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        category_logits, tax_type_logits = outputs\n",
    "                        category_loss = nn.CrossEntropyLoss()(category_logits, labels['category'])\n",
    "                        \n",
    "                        if 'tax_type' in labels:\n",
    "                            tax_type_loss = nn.CrossEntropyLoss()(tax_type_logits, labels['tax_type'])\n",
    "                            loss = 0.7 * category_loss + 0.3 * tax_type_loss\n",
    "                        else:\n",
    "                            loss = category_loss\n",
    "                            \n",
    "                        # Compute accuracy\n",
    "                        preds = category_logits.argmax(dim=1)\n",
    "                        acc = (preds == labels['category']).float().mean().item()\n",
    "                    else:\n",
    "                        # Single task model\n",
    "                        loss = nn.CrossEntropyLoss()(outputs, labels['category'])\n",
    "                        preds = outputs.argmax(dim=1)\n",
    "                        acc = (preds == labels['category']).float().mean().item()\n",
    "                    \n",
    "                # Backward and optimize with mixed precision\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Without mixed precision\n",
    "                outputs = model(\n",
    "                    x=data['x'],\n",
    "                    edge_index=data['edge_index'],\n",
    "                    edge_type=data['edge_type'],\n",
    "                    edge_attr=data['edge_attr'],\n",
    "                    seq_features=data['seq_features'],\n",
    "                    timestamps=data['timestamps'],\n",
    "                    tabular_features=data['tabular_features'],\n",
    "                    t0=data['t0'],\n",
    "                    t1=data['t1'],\n",
    "                    company_features=data['company_features'],\n",
    "                    company_ids=data['company_ids'],\n",
    "                    batch_size=data['batch_size'],\n",
    "                    seq_len=data['seq_len']\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                if isinstance(outputs, tuple):\n",
    "                    category_logits, tax_type_logits = outputs\n",
    "                    category_loss = nn.CrossEntropyLoss()(category_logits, labels['category'])\n",
    "                    \n",
    "                    if 'tax_type' in labels:\n",
    "                        tax_type_loss = nn.CrossEntropyLoss()(tax_type_logits, labels['tax_type'])\n",
    "                        loss = 0.7 * category_loss + 0.3 * tax_type_loss\n",
    "                    else:\n",
    "                        loss = category_loss\n",
    "                        \n",
    "                    # Compute accuracy\n",
    "                    preds = category_logits.argmax(dim=1)\n",
    "                    acc = (preds == labels['category']).float().mean().item()\n",
    "                else:\n",
    "                    # Single task model\n",
    "                    loss = nn.CrossEntropyLoss()(outputs, labels['category'])\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    acc = (preds == labels['category']).float().mean().item()\n",
    "                \n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item() * len(batch_indices)\n",
    "            epoch_acc += acc * len(batch_indices)\n",
    "            samples_processed += len(batch_indices)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if step % config.log_steps == 0:\n",
    "                pbar.set_description(\n",
    "                    f\"Train Loss: {epoch_loss / samples_processed:.4f}, \"\n",
    "                    f\"Acc: {epoch_acc / samples_processed:.4f}\"\n",
    "                )\n",
    "            \n",
    "            # Validation during epoch\n",
    "            if step > 0 and step % config.eval_steps == 0:\n",
    "                # Switch to eval mode\n",
    "                model.eval()\n",
    "                \n",
    "                # Initialize CUDA graph for faster inference if needed\n",
    "                if config.use_cuda_graphs and cuda_graph is None and torch.cuda.is_available():\n",
    "                    cuda_graph = create_cuda_graph(model, data)\n",
    "                \n",
    "                val_loss, val_acc = evaluate(model, val_loader, val_dataset, cuda_graph)\n",
    "                \n",
    "                print(f\"Step {step}/{len(train_loader)}, \"\n",
    "                      f\"Train Loss: {epoch_loss / samples_processed:.4f}, \"\n",
    "                      f\"Train Acc: {epoch_acc / samples_processed:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Val Acc: {val_acc:.4f}\")\n",
    "                \n",
    "                # Switch back to train mode\n",
    "                model.train()\n",
    "        \n",
    "        # End of epoch evaluation\n",
    "        model.eval()\n",
    "        val_loss, val_acc = evaluate(model, val_loader, val_dataset, cuda_graph)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = epoch_loss / samples_processed\n",
    "        train_acc = epoch_acc / samples_processed\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                    'config': config.__dict__\n",
    "                },\n",
    "                os.path.join(config.output_dir, 'best_model.pt')\n",
    "            )\n",
    "            \n",
    "            print(f\"Saved new best model at epoch {epoch + 1} with validation loss {val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs. Best val loss: {best_val_loss:.4f} at epoch {best_epoch + 1}\")\n",
    "            \n",
    "            if patience_counter >= config.patience:\n",
    "                print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'train_accs': train_accs,\n",
    "                'val_accs': val_accs,\n",
    "                'config': config.__dict__\n",
    "            },\n",
    "            os.path.join(config.output_dir, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, dataset, cuda_graph=None):\n",
    "    \"\"\"Evaluate the model on the given dataset\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    samples_processed = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_indices in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
    "            # Get batch dataframe\n",
    "            batch_df = dataset.get_batch_df(batch_indices)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            data, labels = prepare_model_inputs(batch_df, model)\n",
    "            \n",
    "            # Forward pass\n",
    "            if cuda_graph is not None and config.use_cuda_graphs:\n",
    "                # Use CUDA graph for faster inference\n",
    "                outputs = run_with_cuda_graph(cuda_graph, data)\n",
    "            else:\n",
    "                # Regular forward pass\n",
    "                outputs = model(\n",
    "                    x=data['x'],\n",
    "                    edge_index=data['edge_index'],\n",
    "                    edge_type=data['edge_type'],\n",
    "                    edge_attr=data['edge_attr'],\n",
    "                    seq_features=data['seq_features'],\n",
    "                    timestamps=data['timestamps'],\n",
    "                    tabular_features=data['tabular_features'],\n",
    "                    t0=data['t0'],\n",
    "                    t1=data['t1'],\n",
    "                    company_features=data['company_features'],\n",
    "                    company_ids=data['company_ids'],\n",
    "                    batch_size=data['batch_size'],\n",
    "                    seq_len=data['seq_len']\n",
    "                )\n",
    "            \n",
    "            # Compute loss\n",
    "            if isinstance(outputs, tuple):\n",
    "                category_logits, tax_type_logits = outputs\n",
    "                category_loss = nn.CrossEntropyLoss()(category_logits, labels['category'])\n",
    "                \n",
    "                if 'tax_type' in labels:\n",
    "                    tax_type_loss = nn.CrossEntropyLoss()(tax_type_logits, labels['tax_type'])\n",
    "                    loss = 0.7 * category_loss + 0.3 * tax_type_loss\n",
    "                else:\n",
    "                    loss = category_loss\n",
    "                    \n",
    "                # Compute accuracy\n",
    "                preds = category_logits.argmax(dim=1)\n",
    "                acc = (preds == labels['category']).float().mean().item()\n",
    "            else:\n",
    "                # Single task model\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels['category'])\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                acc = (preds == labels['category']).float().mean().item()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item() * len(batch_indices)\n",
    "            total_acc += acc * len(batch_indices)\n",
    "            samples_processed += len(batch_indices)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / samples_processed\n",
    "    avg_acc = total_acc / samples_processed\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings for XGBoost Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from trained model\n",
    "def extract_embeddings_for_xgboost(model, dataset, output_file):\n",
    "    \"\"\"Extract node embeddings for XGBoost integration\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    transaction_ids = []\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        prefetch_factor=config.prefetch_factor,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_indices in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            # Get batch dataframe\n",
    "            batch_df = dataset.get_batch_df(batch_indices)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            data, labels = prepare_model_inputs(batch_df, model)\n",
    "            \n",
    "            # Extract embeddings\n",
    "            embeddings = model.extract_embeddings(data)\n",
    "            \n",
    "            # Store embeddings and labels\n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "            labels_list.append(labels['category'].cpu().numpy())\n",
    "            \n",
    "            if 'txn_id' in batch_df.columns:\n",
    "                transaction_ids.extend(batch_df['txn_id'].tolist())\n",
    "    \n",
    "    # Concatenate embeddings and labels\n",
    "    embeddings_array = np.vstack(embeddings_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    \n",
    "    # Create DataFrame with embeddings\n",
    "    embeddings_df = pd.DataFrame(embeddings_array)\n",
    "    embeddings_df.columns = [f'embedding_{i}' for i in range(embeddings_array.shape[1])]\n",
    "    \n",
    "    # Add labels\n",
    "    embeddings_df['category_id'] = labels_array\n",
    "    \n",
    "    # Add transaction IDs if available\n",
    "    if transaction_ids:\n",
    "        embeddings_df['txn_id'] = transaction_ids\n",
    "    \n",
    "    # Save to file\n",
    "    embeddings_df.to_pickle(os.path.join(config.output_dir, output_file))\n",
    "    \n",
    "    print(f\"Saved {len(embeddings_df)} embeddings to {output_file}\")\n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a small batch to get category and tax type counts\n",
    "print(\"Getting metadata from sample batch...\")\n",
    "sample_df = train_dataset.get_sample_batch(sample_size=100)\n",
    "\n",
    "num_categories = sample_df['category_id'].nunique() if 'category_id' in sample_df.columns else 0\n",
    "num_tax_types = sample_df['tax_type_id'].nunique() if 'tax_type_id' in sample_df.columns else 1\n",
    "\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "print(f\"Number of unique tax types: {num_tax_types}\")\n",
    "\n",
    "# If no categories were found in the sample, use reasonable defaults\n",
    "if num_categories == 0:\n",
    "    print(\"Warning: No categories found in sample. Using default value of 400.\")\n",
    "    num_categories = 400\n",
    "\n",
    "if num_tax_types == 0:\n",
    "    print(\"Warning: No tax types found in sample. Using default value of 20.\")\n",
    "    num_tax_types = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parquet files\n",
    "parquet_files = get_parquet_files(config.data_dir, config.max_files)\n",
    "\n",
    "if not parquet_files:\n",
    "    raise ValueError(f\"No parquet files found in {config.data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split files into train and validation sets\n",
    "train_files, val_files = train_test_split(parquet_files, test_size=0.2, random_state=42)\n",
    "print(f\"Training on {len(train_files)} files, validating on {len(val_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ParquetTransactionDataset(train_files, preprocess_fn=preprocess_transactions)\n",
    "val_dataset = ParquetTransactionDataset(val_files, preprocess_fn=preprocess_transactions)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} transactions\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch to get category and tax type counts\n",
    "sample_batch_indices = list(range(min(1000, len(train_dataset))))\n",
    "sample_df = train_dataset.get_batch_df(sample_batch_indices)\n",
    "\n",
    "num_categories = sample_df['category_id'].nunique()\n",
    "num_tax_types = sample_df['tax_type_id'].nunique() if 'tax_type_id' in sample_df.columns else 1\n",
    "\n",
    "print(f\"Number of unique categories: {num_categories}\")\n",
    "print(f\"Number of unique tax types: {num_tax_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = initialize_model(config.hidden_dim, num_categories, num_tax_types)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "training_history = train(model, train_dataset, val_dataset, config)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "print(f\"Best epoch: {training_history['best_epoch'] + 1} with validation loss {training_history['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history['train_losses'], label='Train Loss')\n",
    "plt.plot(training_history['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history['train_accs'], label='Train Accuracy')\n",
    "plt.plot(training_history['val_accs'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, 'training_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(config.output_dir, 'best_model.pt'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1} with validation loss {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for XGBoost if requested\n",
    "if config.extract_embeddings:\n",
    "    print(\"Extracting embeddings for XGBoost integration...\")\n",
    "    embeddings_df = extract_embeddings_for_xgboost(model, train_dataset, config.embedding_output_file)\n",
    "    \n",
    "    # Show sample embeddings\n",
    "    print(\"\\nSample embeddings:\")\n",
    "    display(embeddings_df.head())\n",
    "    \n",
    "    # Show embedding dimensions\n",
    "    embedding_cols = [col for col in embeddings_df.columns if col.startswith('embedding_')]\n",
    "    print(f\"\\nEmbedding dimension: {len(embedding_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Analysis and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory analysis\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nGPU Memory Analysis:\")\n",
    "    print(f\"Peak Memory Allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Peak Memory Reserved: {torch.cuda.max_memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Reset peak stats\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The EnhancedHybridTransactionModel has been successfully trained on multiple parquet files using GPU acceleration on a p3.2xlarge instance. This model combines graph-based transaction relationships with temporal patterns and company-based grouping to improve classification accuracy.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Use the extracted embeddings as features for XGBoost models\n",
    "2. Experiment with different graph relationship types and weights\n",
    "3. Integrate with existing ML pipelines\n",
    "4. Deploy the model for real-time inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
