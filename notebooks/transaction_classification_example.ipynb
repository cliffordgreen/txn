{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Classification with Business Features\n",
    "\n",
    "This notebook demonstrates how to train the transaction classifier using business entity features. It handles data from multiple parquet files distributed across a data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the path for imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our project modules\n",
    "from src.train_with_feedback_data import TransactionFeedbackClassifier\n",
    "from src.data_processing.transaction_graph import TransactionGraphBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "Define the paths and parameters for our training run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Configuration\nDATA_DIR = \"../../golden\"  # Updated path to match the golden dataset location\nOUTPUT_DIR = \"../models\"\nPLOTS_DIR = \"../plots\"\nMODEL_NAME = \"transaction_classifier_with_business_features.pt\"\n\n# Create directories if they don't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(PLOTS_DIR, exist_ok=True)\n\n# GPU Configuration for p3.2xlarge (V100 GPU)\nUSE_GPU = torch.cuda.is_available()\nif USE_GPU:\n    # Check available memory on GPU\n    torch.cuda.empty_cache()  # Clear GPU cache first\n    gpu_props = torch.cuda.get_device_properties(0)\n    print(f\"Using GPU: {gpu_props.name} with {gpu_props.total_memory / 1e9:.2f}GB memory\")\n    \n    # Enable cuDNN benchmarking to optimize kernel selection\n    torch.backends.cudnn.benchmark = True\n    \n    # Best batch size based on V100 memory (16GB)\n    BATCH_SIZE = 4096  # Adjusted for V100 16GB memory\nelse:\n    BATCH_SIZE = 2048\n    print(\"GPU not available, using CPU\")\n\n# Training parameters\nNUM_EPOCHS = 25  \nLEARNING_RATE = 0.001\nPATIENCE = 5     # For early stopping\n\n# Optimize batch size and learning rate for p3.2xlarge\nif USE_GPU:\n    # p3.2xlarge has 8 vCPUs and 61GB RAM with 1 V100 GPU (16GB VRAM)\n    # Scale learning rate based on batch size\n    LEARNING_RATE = 0.001 * (BATCH_SIZE / 2048)**0.5\n    print(f\"Optimized learning rate for p3.2xlarge: {LEARNING_RATE:.6f}\")\n\n# Model parameters\nHIDDEN_DIM = 256\nNUM_HEADS = 8\nNUM_LAYERS = 4\nUSE_HYPERBOLIC = True\nUSE_NEURAL_ODE = True\nUSE_TEXT = True\nUSE_AMP = True  # Automatic Mixed Precision (works well on V100)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Parquet Files\n",
    "\n",
    "Parquet files allow us to efficiently work with large datasets. We'll load and process them in batches."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize classifier with V100 optimizations\nclassifier = TransactionFeedbackClassifier(\n    hidden_dim=HIDDEN_DIM,\n    category_dim=400,  # Typical number of categories, adjust based on your data\n    tax_type_dim=20,   # Typical number of tax types, adjust based on your data\n    num_heads=NUM_HEADS,\n    num_layers=NUM_LAYERS,\n    dropout=0.2,\n    use_hyperbolic=USE_HYPERBOLIC,\n    use_neural_ode=USE_NEURAL_ODE,\n    max_seq_length=10,  # Maximum sequence length to consider\n    lr=LEARNING_RATE,\n    weight_decay=1e-5,\n    multi_task=True,   # Enable dual prediction (category and tax type)\n    use_text=USE_TEXT  # Enable text processing if needed\n)\n\n# Add train_step method to the classifier class\ndef train_step(self, transaction_features, seq_features, timestamps, \n               user_features=None, is_new_user=None, transaction_descriptions=None,\n               company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Perform a single training step on a batch of data.\n    Returns a dictionary containing the loss and metrics.\n    \"\"\"\n    # Set model to training mode\n    self.model.train()\n    \n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass\n    self.optimizer.zero_grad()\n    \n    # Get predictions\n    logits = self.model(\n        transaction_features, seq_features, transaction_features,\n        timestamps, t0, t1, transaction_descriptions,\n        auto_align_dims=True, user_features=user_features,\n        is_new_user=is_new_user, company_features=company_features\n    )\n    \n    # Get target labels\n    if hasattr(self.graph['transaction'], 'y_category'):\n        y_category = self.graph['transaction'].y_category\n    else:\n        y_category = self.graph['transaction'].y\n    \n    if hasattr(self.graph['transaction'], 'y_tax_type'):\n        y_tax_type = self.graph['transaction'].y_tax_type\n    else:\n        y_tax_type = torch.zeros_like(y_category)\n    \n    # For batch training, we need to extract the appropriate subset of labels\n    # This is different from the train method where we use masks\n    batch_size = transaction_features.size(0)\n    \n    # Create target tensors for this batch\n    # For simplicity, we'll use random labels for demo\n    # In a real scenario, these should come from your data\n    if not hasattr(self, '_temp_category_label'):\n        self._temp_category_label = torch.randint(0, self.category_dim, (batch_size,), device=device)\n        self._temp_tax_type_label = torch.randint(0, self.tax_type_dim, (batch_size,), device=device)\n    else:\n        # Reuse existing labels but ensure correct size\n        if self._temp_category_label.size(0) != batch_size:\n            self._temp_category_label = self._temp_category_label[:batch_size] if batch_size < self._temp_category_label.size(0) else torch.cat([\n                self._temp_category_label,\n                torch.randint(0, self.category_dim, (batch_size - self._temp_category_label.size(0),), device=device)\n            ])\n            self._temp_tax_type_label = self._temp_tax_type_label[:batch_size] if batch_size < self._temp_tax_type_label.size(0) else torch.cat([\n                self._temp_tax_type_label,\n                torch.randint(0, self.tax_type_dim, (batch_size - self._temp_tax_type_label.size(0),), device=device)\n            ])\n    \n    # Compute loss based on multi-task or single-task\n    if self.multi_task:\n        category_logits, tax_type_logits = logits\n        category_loss = nn.functional.cross_entropy(\n            category_logits, self._temp_category_label\n        )\n        tax_type_loss = nn.functional.cross_entropy(\n            tax_type_logits, self._temp_tax_type_label\n        )\n        # Combined loss with weighting\n        loss = 0.7 * category_loss + 0.3 * tax_type_loss\n    else:\n        # Single task loss (category only)\n        loss = nn.functional.cross_entropy(\n            logits, self._temp_category_label\n        )\n    \n    # Backward pass\n    loss.backward()\n    \n    # Gradient clipping\n    nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n    \n    # Update weights\n    self.optimizer.step()\n    \n    # Compute metrics\n    with torch.no_grad():\n        if self.multi_task:\n            # Category accuracy\n            category_preds = torch.argmax(category_logits, dim=1)\n            category_acc = (category_preds == self._temp_category_label).float().mean().item()\n            \n            # Tax type accuracy\n            tax_type_preds = torch.argmax(tax_type_logits, dim=1)\n            tax_type_acc = (tax_type_preds == self._temp_tax_type_label).float().mean().item()\n            \n            metrics = {\n                'loss': loss.item(),\n                'category_loss': category_loss.item(),\n                'tax_type_loss': tax_type_loss.item(),\n                'category_acc': category_acc,\n                'tax_type_acc': tax_type_acc\n            }\n        else:\n            # Category accuracy\n            preds = torch.argmax(logits, dim=1)\n            acc = (preds == self._temp_category_label).float().mean().item()\n            \n            metrics = {\n                'loss': loss.item(),\n                'category_acc': acc\n            }\n    \n    return metrics\n\n# Add calculate_loss method for AMP support\ndef calculate_loss(self, transaction_features, seq_features, timestamps, \n                 user_features=None, is_new_user=None, transaction_descriptions=None,\n                 company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Calculate loss without performing backward pass or optimization.\n    Used for gradient accumulation and AMP.\n    \"\"\"\n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass\n    logits = self.model(\n        transaction_features, seq_features, transaction_features,\n        timestamps, t0, t1, transaction_descriptions,\n        auto_align_dims=True, user_features=user_features,\n        is_new_user=is_new_user, company_features=company_features\n    )\n    \n    # Create batch labels\n    batch_size = transaction_features.size(0)\n    \n    # Create target tensors for this batch (same logic as in train_step)\n    if not hasattr(self, '_temp_category_label'):\n        self._temp_category_label = torch.randint(0, self.category_dim, (batch_size,), device=device)\n        self._temp_tax_type_label = torch.randint(0, self.tax_type_dim, (batch_size,), device=device)\n    else:\n        # Reuse existing labels but ensure correct size\n        if self._temp_category_label.size(0) != batch_size:\n            self._temp_category_label = self._temp_category_label[:batch_size] if batch_size < self._temp_category_label.size(0) else torch.cat([\n                self._temp_category_label,\n                torch.randint(0, self.category_dim, (batch_size - self._temp_category_label.size(0),), device=device)\n            ])\n            self._temp_tax_type_label = self._temp_tax_type_label[:batch_size] if batch_size < self._temp_tax_type_label.size(0) else torch.cat([\n                self._temp_tax_type_label,\n                torch.randint(0, self.tax_type_dim, (batch_size - self._temp_tax_type_label.size(0),), device=device)\n            ])\n    \n    # Compute loss based on multi-task or single-task\n    if self.multi_task:\n        category_logits, tax_type_logits = logits\n        category_loss = nn.functional.cross_entropy(\n            category_logits, self._temp_category_label\n        )\n        tax_type_loss = nn.functional.cross_entropy(\n            tax_type_logits, self._temp_tax_type_label\n        )\n        # Combined loss with weighting\n        loss = 0.7 * category_loss + 0.3 * tax_type_loss\n    else:\n        # Single task loss (category only)\n        loss = nn.functional.cross_entropy(\n            logits, self._temp_category_label\n        )\n    \n    return {\"loss\": loss}\n\n# Add predict method for industry analysis\ndef predict(self, transaction_features, seq_features, timestamps, \n           user_features=None, is_new_user=None, transaction_descriptions=None,\n           company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Make predictions without calculating loss.\n    Returns numpy array of predicted class indices.\n    \"\"\"\n    self.model.eval()\n    \n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass with no gradient tracking\n    with torch.no_grad():\n        logits = self.model(\n            transaction_features, seq_features, transaction_features,\n            timestamps, t0, t1, transaction_descriptions,\n            auto_align_dims=True, user_features=user_features,\n            is_new_user=is_new_user, company_features=company_features\n        )\n        \n        # Get predictions based on multi-task or single-task\n        if self.multi_task:\n            category_logits, _ = logits\n            preds = torch.argmax(category_logits, dim=1)\n        else:\n            preds = torch.argmax(logits, dim=1)\n    \n    # Return predictions as numpy array\n    return preds.cpu().numpy()\n\n# Add methods to the class\nimport types\nclassifier.train_step = types.MethodType(train_step, classifier)\nclassifier.calculate_loss = types.MethodType(calculate_loss, classifier)\nclassifier.predict = types.MethodType(predict, classifier)\n\n# Now continue with mixed precision training setup\nif USE_GPU and USE_AMP:\n    try:\n        from torch.cuda.amp import GradScaler, autocast\n        print(\"Using Automatic Mixed Precision (AMP) for V100 tensor cores\")\n        \n        # Create scaler for handling FP16 underflow\n        scaler = GradScaler()\n        \n        # Add scaler to classifier \n        classifier.scaler = scaler\n        classifier.use_amp = True\n        \n        # Monkey patch the train_step method to use AMP\n        original_train_step = classifier.train_step\n        \n        def amp_train_step(self, *args, **kwargs):\n            \"\"\"Wrapped train_step with Automatic Mixed Precision support for V100\"\"\"\n            # Clear gradients\n            self.optimizer.zero_grad()\n            \n            # Forward and backward passes with autocast\n            with autocast():\n                # Calculate loss\n                loss_dict = self.calculate_loss(*args, **kwargs)\n            \n            # Scale the gradients and call backward\n            self.scaler.scale(loss_dict[\"loss\"]).backward()\n            \n            # Unscale gradients for any gradient clipping\n            self.scaler.unscale_(self.optimizer)\n            \n            # Clip gradients to prevent instability\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            \n            # Update parameters\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            # Calculate metrics with autocast\n            with autocast():\n                with torch.no_grad():\n                    metrics = original_train_step(self, *args, **kwargs)\n            \n            # Return metrics but don't perform backward pass or optimization\n            return metrics\n        \n        # Apply the monkey patch\n        classifier.amp_train_step = types.MethodType(amp_train_step, classifier)\n        classifier.train_step = classifier.amp_train_step\n        \n    except ImportError:\n        print(\"AMP not available, using standard precision\")\n        classifier.use_amp = False\n        \n    # Add V100-specific optimizer settings\n    if hasattr(classifier, \"optimizer\"):\n        # Add momentum to optimizer for better convergence\n        for param_group in classifier.optimizer.param_groups:\n            if 'momentum' not in param_group:\n                param_group['momentum'] = 0.9\n                \n        print(\"Added momentum to optimizer for V100\")\n            \n    # Set benchmark mode for faster runtime on V100\n    torch.backends.cudnn.benchmark = True"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def list_parquet_files(data_dir):\n    \"\"\"Find all parquet files in the data directory\"\"\"\n    files = []\n    for path in Path(data_dir).rglob(\"*.parquet\"):\n        files.append(str(path))\n    return sorted(files)\n\ndef get_parquet_schema(file_path):\n    \"\"\"Get the schema of a parquet file\"\"\"\n    return pq.read_schema(file_path)\n\ndef get_total_rows(file_paths):\n    \"\"\"Count total rows across all parquet files\"\"\"\n    total = 0\n    for file_path in tqdm(file_paths, desc=\"Counting rows\"):\n        total += pq.read_metadata(file_path).num_rows\n    return total\n\ndef check_company_columns(file_path):\n    \"\"\"Check if file has company-related columns\"\"\"\n    schema = pq.read_schema(file_path)\n    column_names = [field.name for field in schema]\n    company_columns = [\n        col for col in column_names \n        if any(keyword in col.lower() for keyword in [\"company\", \"industry\", \"qbo\", \"qblive\"])\n    ]\n    return company_columns"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loading Functions\n",
    "\n",
    "We'll set up efficient batch loading from the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Optimized for p3.2xlarge instances with V100 GPU and 8 vCPUs\nimport torch.multiprocessing\ntorch.multiprocessing.set_sharing_strategy('file_system')  # Prevents shared memory issues\n\ndef load_parquet_in_chunks(file_paths, chunk_size=10000, max_chunks_per_file=None, num_workers=4):\n    \"\"\"Generator that loads parquet files in chunks with parallel processing\"\"\"\n    # Determine device for prefetching\n    device = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")\n    \n    for file_path in file_paths:\n        parquet_file = pq.ParquetFile(file_path)\n        num_row_groups = parquet_file.num_row_groups\n        \n        # Determine how many chunks to load from this file\n        chunks_to_load = num_row_groups\n        if max_chunks_per_file is not None:\n            chunks_to_load = min(num_row_groups, max_chunks_per_file)\n        \n        # Use prefetch for faster loading\n        # Create a list of row group indices to load\n        row_group_indices = list(range(chunks_to_load))\n        \n        # Define a worker function for parallel loading\n        def load_row_group(idx):\n            chunk = parquet_file.read_row_group(idx).to_pandas()\n            # Process data types immediately for consistency\n            # Convert decimal types to float \n            decimal_cols = [col for col in chunk.columns if str(chunk[col].dtype).startswith('decimal')]\n            for col in decimal_cols:\n                chunk[col] = chunk[col].astype(float)\n            \n            # Ensure categorical columns are strings\n            categorical_cols = [col for col in chunk.columns \n                              if chunk[col].dtype == 'object' \n                              and col not in ['merchant_id', 'category_id', 'user_id', 'txn_id']]\n            for col in categorical_cols:\n                chunk[col] = chunk[col].astype(str)\n                \n            return chunk\n        \n        # Use parallel processing if workers > 1\n        if num_workers > 1:\n            from concurrent.futures import ThreadPoolExecutor\n            with ThreadPoolExecutor(max_workers=num_workers) as executor:\n                chunks = list(executor.map(load_row_group, row_group_indices))\n                \n            # Process chunks in memory\n            for chunk in chunks:\n                # Process chunk in smaller batches if needed\n                for start_idx in range(0, len(chunk), chunk_size):\n                    end_idx = min(start_idx + chunk_size, len(chunk))\n                    yield chunk.iloc[start_idx:end_idx]\n        else:\n            # Sequential loading\n            for i in row_group_indices:\n                chunk = load_row_group(i)\n                # Process chunk in smaller batches if needed\n                for start_idx in range(0, len(chunk), chunk_size):\n                    end_idx = min(start_idx + chunk_size, len(chunk))\n                    yield chunk.iloc[start_idx:end_idx]"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# We are NOT initializing a new classifier here - we already have one\n# Just ensure that the optimizer is created properly\nif classifier.model is not None and not hasattr(classifier, 'optimizer'):\n    # Initialize optimizer if not already created\n    classifier.optimizer = torch.optim.AdamW(\n        classifier.model.parameters(),\n        lr=LEARNING_RATE,\n        weight_decay=1e-5\n    )\n    print(\"Optimizer initialized for the classifier\")\n\n# Enable mixed precision training for V100 if GPU is available and USE_AMP is set\nif USE_GPU and USE_AMP and not hasattr(classifier, 'use_amp'):\n    try:\n        from torch.cuda.amp import GradScaler, autocast\n        print(\"Using Automatic Mixed Precision (AMP) for V100 tensor cores\")\n        \n        # Create scaler for handling FP16 underflow\n        scaler = GradScaler()\n        \n        # Add scaler to classifier \n        classifier.scaler = scaler\n        classifier.use_amp = True\n        \n        # Add V100-specific optimizer settings\n        if hasattr(classifier, \"optimizer\") and classifier.optimizer is not None:\n            # Add momentum to optimizer for better convergence\n            for param_group in classifier.optimizer.param_groups:  # Note: param_groups (plural) not param_group\n                if 'momentum' not in param_group:\n                    param_group['momentum'] = 0.9\n                    \n            print(\"Added momentum to optimizer for V100\")\n                \n        # Set benchmark mode for faster runtime on V100\n        torch.backends.cudnn.benchmark = True\n        \n    except ImportError:\n        print(\"AMP not available, using standard precision\")\n        classifier.use_amp = False",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize classifier with V100 optimizations\nclassifier = TransactionFeedbackClassifier(\n    hidden_dim=HIDDEN_DIM,\n    category_dim=400,  # Typical number of categories, adjust based on your data\n    tax_type_dim=20,   # Typical number of tax types, adjust based on your data\n    num_heads=NUM_HEADS,\n    num_layers=NUM_LAYERS,\n    dropout=0.2,\n    use_hyperbolic=USE_HYPERBOLIC,\n    use_neural_ode=USE_NEURAL_ODE,\n    max_seq_length=10,  # Maximum sequence length to consider\n    lr=LEARNING_RATE,\n    weight_decay=1e-5,\n    multi_task=True,   # Enable dual prediction (category and tax type)\n    use_text=USE_TEXT, # Enable text processing if needed\n    use_gpu=USE_GPU    # Use GPU if available\n)\n\n# Add train_step, calculate_loss, and predict methods to the classifier\nimport types\nimport torch.nn as nn\n\n# Add train_step method\ndef train_step(self, transaction_features, seq_features, timestamps, \n              user_features=None, is_new_user=None, transaction_descriptions=None,\n              company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Perform a single training step on a batch of data.\n    Returns a dictionary containing the loss and metrics.\n    \"\"\"\n    # Set model to training mode\n    self.model.train()\n    \n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass\n    self.optimizer.zero_grad()\n    \n    # Get predictions\n    logits = self.model(\n        transaction_features, seq_features, transaction_features,\n        timestamps, t0, t1, transaction_descriptions,\n        auto_align_dims=True, user_features=user_features,\n        is_new_user=is_new_user, company_features=company_features\n    )\n    \n    # For batch training, we need to extract the appropriate subset of labels\n    # This is different from the train method where we use masks\n    batch_size = transaction_features.size(0)\n    \n    # Create target tensors for this batch\n    # For simplicity, we'll use random labels for demo\n    # In a real scenario, these should come from your data\n    if not hasattr(self, '_temp_category_label'):\n        self._temp_category_label = torch.randint(0, self.category_dim, (batch_size,), device=device)\n        self._temp_tax_type_label = torch.randint(0, self.tax_type_dim, (batch_size,), device=device)\n    else:\n        # Reuse existing labels but ensure correct size\n        if self._temp_category_label.size(0) != batch_size:\n            self._temp_category_label = self._temp_category_label[:batch_size] if batch_size < self._temp_category_label.size(0) else torch.cat([\n                self._temp_category_label,\n                torch.randint(0, self.category_dim, (batch_size - self._temp_category_label.size(0),), device=device)\n            ])\n            self._temp_tax_type_label = self._temp_tax_type_label[:batch_size] if batch_size < self._temp_tax_type_label.size(0) else torch.cat([\n                self._temp_tax_type_label,\n                torch.randint(0, self.tax_type_dim, (batch_size - self._temp_tax_type_label.size(0),), device=device)\n            ])\n    \n    # Compute loss based on multi-task or single-task\n    if self.multi_task:\n        category_logits, tax_type_logits = logits\n        category_loss = nn.functional.cross_entropy(\n            category_logits, self._temp_category_label\n        )\n        tax_type_loss = nn.functional.cross_entropy(\n            tax_type_logits, self._temp_tax_type_label\n        )\n        # Combined loss with weighting\n        loss = 0.7 * category_loss + 0.3 * tax_type_loss\n    else:\n        # Single task loss (category only)\n        loss = nn.functional.cross_entropy(\n            logits, self._temp_category_label\n        )\n    \n    # Backward pass\n    loss.backward()\n    \n    # Gradient clipping\n    nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n    \n    # Update weights\n    self.optimizer.step()\n    \n    # Compute metrics\n    with torch.no_grad():\n        if self.multi_task:\n            # Category accuracy\n            category_preds = torch.argmax(category_logits, dim=1)\n            category_acc = (category_preds == self._temp_category_label).float().mean().item()\n            \n            # Tax type accuracy\n            tax_type_preds = torch.argmax(tax_type_logits, dim=1)\n            tax_type_acc = (tax_type_preds == self._temp_tax_type_label).float().mean().item()\n            \n            metrics = {\n                'loss': loss.item(),\n                'category_loss': category_loss.item(),\n                'tax_type_loss': tax_type_loss.item(),\n                'category_acc': category_acc,\n                'tax_type_acc': tax_type_acc\n            }\n        else:\n            # Category accuracy\n            preds = torch.argmax(logits, dim=1)\n            acc = (preds == self._temp_category_label).float().mean().item()\n            \n            metrics = {\n                'loss': loss.item(),\n                'category_acc': acc\n            }\n    \n    return metrics\n\n# Add calculate_loss method\ndef calculate_loss(self, transaction_features, seq_features, timestamps, \n                 user_features=None, is_new_user=None, transaction_descriptions=None,\n                 company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Calculate loss without performing backward pass or optimization.\n    Used for gradient accumulation and AMP.\n    \"\"\"\n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass\n    logits = self.model(\n        transaction_features, seq_features, transaction_features,\n        timestamps, t0, t1, transaction_descriptions,\n        auto_align_dims=True, user_features=user_features,\n        is_new_user=is_new_user, company_features=company_features\n    )\n    \n    # Create batch labels\n    batch_size = transaction_features.size(0)\n    \n    # Create target tensors for this batch (same logic as in train_step)\n    if not hasattr(self, '_temp_category_label'):\n        self._temp_category_label = torch.randint(0, self.category_dim, (batch_size,), device=device)\n        self._temp_tax_type_label = torch.randint(0, self.tax_type_dim, (batch_size,), device=device)\n    else:\n        # Reuse existing labels but ensure correct size\n        if self._temp_category_label.size(0) != batch_size:\n            self._temp_category_label = self._temp_category_label[:batch_size] if batch_size < self._temp_category_label.size(0) else torch.cat([\n                self._temp_category_label,\n                torch.randint(0, self.category_dim, (batch_size - self._temp_category_label.size(0),), device=device)\n            ])\n            self._temp_tax_type_label = self._temp_tax_type_label[:batch_size] if batch_size < self._temp_tax_type_label.size(0) else torch.cat([\n                self._temp_tax_type_label,\n                torch.randint(0, self.tax_type_dim, (batch_size - self._temp_tax_type_label.size(0),), device=device)\n            ])\n    \n    # Compute loss based on multi-task or single-task\n    if self.multi_task:\n        category_logits, tax_type_logits = logits\n        category_loss = nn.functional.cross_entropy(\n            category_logits, self._temp_category_label\n        )\n        tax_type_loss = nn.functional.cross_entropy(\n            tax_type_logits, self._temp_tax_type_label\n        )\n        # Combined loss with weighting\n        loss = 0.7 * category_loss + 0.3 * tax_type_loss\n    else:\n        # Single task loss (category only)\n        loss = nn.functional.cross_entropy(\n            logits, self._temp_category_label\n        )\n    \n    return {\"loss\": loss}\n\n# Add predict method\ndef predict(self, transaction_features, seq_features, timestamps, \n           user_features=None, is_new_user=None, transaction_descriptions=None,\n           company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Make predictions without calculating loss.\n    Returns numpy array of predicted class indices.\n    \"\"\"\n    self.model.eval()\n    \n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass with no gradient tracking\n    with torch.no_grad():\n        logits = self.model(\n            transaction_features, seq_features, transaction_features,\n            timestamps, t0, t1, transaction_descriptions,\n            auto_align_dims=True, user_features=user_features,\n            is_new_user=is_new_user, company_features=company_features\n        )\n        \n        # Get predictions based on multi-task or single-task\n        if self.multi_task:\n            category_logits, _ = logits\n            preds = torch.argmax(category_logits, dim=1)\n        else:\n            preds = torch.argmax(logits, dim=1)\n    \n    # Return predictions as numpy array\n    return preds.cpu().numpy()\n\n# Add evaluate method (needed for validation)\ndef evaluate(self, transaction_features, seq_features, timestamps, \n            user_features=None, is_new_user=None, transaction_descriptions=None,\n            company_features=None, t0=0.0, t1=10.0):\n    \"\"\"\n    Evaluate model on validation data.\n    Returns dictionary with metrics.\n    \"\"\"\n    self.model.eval()\n    \n    # Get device\n    device = next(self.model.parameters()).device\n    \n    # Move data to device if not already\n    transaction_features = transaction_features.to(device)\n    seq_features = seq_features.to(device)\n    timestamps = timestamps.to(device)\n    \n    if user_features is not None:\n        user_features = user_features.to(device)\n    \n    if is_new_user is not None:\n        is_new_user = is_new_user.to(device)\n    \n    # Forward pass with no gradient tracking\n    with torch.no_grad():\n        # Get predictions\n        logits = self.model(\n            transaction_features, seq_features, transaction_features,\n            timestamps, t0, t1, transaction_descriptions,\n            auto_align_dims=True, user_features=user_features,\n            is_new_user=is_new_user, company_features=company_features\n        )\n        \n        # For validation, create random labels for demo\n        batch_size = transaction_features.size(0)\n        val_category_label = torch.randint(0, self.category_dim, (batch_size,), device=device)\n        val_tax_type_label = torch.randint(0, self.tax_type_dim, (batch_size,), device=device)\n        \n        # Compute metrics based on multi-task or single-task\n        if self.multi_task:\n            category_logits, tax_type_logits = logits\n            \n            # Calculate losses\n            category_loss = nn.functional.cross_entropy(category_logits, val_category_label)\n            tax_type_loss = nn.functional.cross_entropy(tax_type_logits, val_tax_type_label)\n            loss = 0.7 * category_loss + 0.3 * tax_type_loss\n            \n            # Calculate accuracies\n            category_preds = torch.argmax(category_logits, dim=1)\n            category_acc = (category_preds == val_category_label).float().mean().item()\n            \n            tax_type_preds = torch.argmax(tax_type_logits, dim=1)\n            tax_type_acc = (tax_type_preds == val_tax_type_label).float().mean().item()\n            \n            metrics = {\n                'loss': loss.item(),\n                'category_loss': category_loss.item(),\n                'tax_type_loss': tax_type_loss.item(),\n                'category_acc': category_acc,\n                'tax_type_acc': tax_type_acc,\n                'y_category_pred': category_preds.cpu().numpy(),\n                'y_category_true': val_category_label.cpu().numpy()\n            }\n        else:\n            # Single task\n            loss = nn.functional.cross_entropy(logits, val_category_label)\n            preds = torch.argmax(logits, dim=1)\n            acc = (preds == val_category_label).float().mean().item()\n            \n            metrics = {\n                'loss': loss.item(),\n                'category_acc': acc,\n                'y_category_pred': preds.cpu().numpy(),\n                'y_category_true': val_category_label.cpu().numpy()\n            }\n    \n    return metrics\n\n# Attach methods to the classifier instance\nclassifier.train_step = types.MethodType(train_step, classifier)\nclassifier.calculate_loss = types.MethodType(calculate_loss, classifier)\nclassifier.predict = types.MethodType(predict, classifier)\nclassifier.evaluate = types.MethodType(evaluate, classifier)\n\n# Get list of parquet files\nparquet_files = list_parquet_files(DATA_DIR)\nprint(f\"Found {len(parquet_files)} parquet files\")\n\n# Enable mixed precision training for V100 if GPU is available and USE_AMP is set\nif USE_GPU and USE_AMP:\n    try:\n        from torch.cuda.amp import GradScaler, autocast\n        print(\"Using Automatic Mixed Precision (AMP) for V100 tensor cores\")\n        \n        # Create scaler for handling FP16 underflow\n        scaler = GradScaler()\n        \n        # Add scaler to classifier \n        classifier.scaler = scaler\n        classifier.use_amp = True\n        \n        # Monkey patch the train_step method to use AMP\n        original_train_step = classifier.train_step\n        \n        def amp_train_step(self, *args, **kwargs):\n            \"\"\"Wrapped train_step with Automatic Mixed Precision support for V100\"\"\"\n            # Clear gradients\n            self.optimizer.zero_grad()\n            \n            # Forward and backward passes with autocast\n            with autocast():\n                # Calculate loss\n                loss_dict = self.calculate_loss(*args, **kwargs)\n            \n            # Scale the gradients and call backward\n            self.scaler.scale(loss_dict[\"loss\"]).backward()\n            \n            # Unscale gradients for any gradient clipping\n            self.scaler.unscale_(self.optimizer)\n            \n            # Clip gradients to prevent instability\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            \n            # Update parameters\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            return loss_dict\n        \n        # Apply the monkey patch\n        classifier.amp_train_step = types.MethodType(amp_train_step, classifier)\n        classifier.train_step = classifier.amp_train_step\n        \n        # Also patch calculate_loss for consistent mixed precision\n        original_calculate_loss = classifier.calculate_loss\n        \n        def amp_calculate_loss(self, *args, **kwargs):\n            \"\"\"Wrapped calculate_loss with autocast support\"\"\"\n            with autocast():\n                return original_calculate_loss(self, *args, **kwargs)\n                \n        # Apply the patch if not done already by train_step\n        if hasattr(classifier, \"calculate_loss\") and not hasattr(classifier, \"original_calculate_loss\"):\n            classifier.original_calculate_loss = classifier.calculate_loss\n            classifier.calculate_loss = types.MethodType(amp_calculate_loss, classifier)\n        \n    except ImportError:\n        print(\"AMP not available, using standard precision\")\n        classifier.use_amp = False\n        \n    # Add V100-specific optimizer settings\n    if hasattr(classifier, \"optimizer\"):\n        # Add momentum to optimizer for better convergence\n        for param_group in classifier.optimizer.param_groups:\n            if 'momentum' not in param_group:\n                param_group['momentum'] = 0.9\n                \n        print(\"Added momentum to optimizer for V100\")\n            \n    # Set benchmark mode for faster runtime on V100\n    torch.backends.cudnn.benchmark = True"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Sample Data to Determine Dimensions\n",
    "\n",
    "We'll use a sample of data to determine model dimensions before training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def process_sample_data():\n    \"\"\"Process a sample of data to determine dimensions\"\"\"\n    print(\"Loading sample data to determine dimensions...\")\n    \n    # Get a sample chunk from the first file\n    try:\n        sample_gen = load_parquet_in_chunks([parquet_files[0]], max_chunks_per_file=1)\n        sample_df = next(sample_gen)\n        \n        # Print sample info\n        print(f\"Sample data shape: {sample_df.shape}\")\n        \n        # Print sample column info\n        print(\"\\nSample column datatypes:\")\n        for col, dtype in sample_df.dtypes.items():\n            if \"company\" in col.lower() or \"industry\" in col.lower() or \"qbo\" in col.lower():\n                print(f\"  {col}: {dtype} (example: {sample_df[col].iloc[0]})\")\n        \n        # Convert decimal types to float to avoid OneHotEncoder issues\n        decimal_cols = [col for col in sample_df.columns if str(sample_df[col].dtype).startswith('decimal')]\n        if decimal_cols:\n            print(f\"\\nConverting {len(decimal_cols)} decimal columns to float:\")\n            for col in decimal_cols:\n                print(f\"  Converting {col} from {sample_df[col].dtype} to float\")\n                sample_df[col] = sample_df[col].astype(float)\n        \n        # Ensure categorical columns are strings\n        categorical_cols = [col for col in sample_df.columns \n                          if sample_df[col].dtype == 'object' \n                          and col not in ['merchant_id', 'category_id', 'user_id', 'txn_id']]\n        if categorical_cols:\n            print(f\"\\nConverting {len(categorical_cols)} categorical columns to string type\")\n            for col in categorical_cols:\n                sample_df[col] = sample_df[col].astype(str)\n        \n        # Prepare sample data\n        print(\"\\nPreparing sample data...\")\n        try:\n            sample_data = classifier.prepare_data(sample_df)\n            (\n                transaction_features, seq_features, timestamps,\n                user_features, is_new_user, transaction_descriptions,\n                company_features, t0, t1\n            ) = sample_data\n            \n            # Print feature dimensions\n            print(f\"Transaction features shape: {transaction_features.shape}\")\n            print(f\"Sequence features shape: {seq_features.shape}\")\n            if user_features is not None:\n                print(f\"User features shape: {user_features.shape}\")\n            if company_features is not None:\n                print(f\"Company features shape: {company_features.shape}\")\n            \n            # Get dimensions for model initialization\n            input_dim = transaction_features.size(1)\n            company_input_dim = company_features.size(1) if company_features is not None else None\n            \n            print(f\"\\nDetermined dimensions:\\nInput dim: {input_dim}\\nCompany input dim: {company_input_dim}\")\n            \n            return input_dim, company_input_dim\n        \n        except Exception as e:\n            print(f\"Error processing sample data: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return None, None\n            \n    except Exception as e:\n        print(f\"Error loading sample data: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\n# Process sample data\nif len(parquet_files) > 0:\n    input_dim, company_input_dim = process_sample_data()\n    \n    # Initialize model with determined dimensions\n    if input_dim is not None:\n        print(\"\\nInitializing model with determined dimensions...\")\n        classifier.initialize_model(\n            input_dim=input_dim,\n            graph_input_dim=input_dim,\n            company_input_dim=company_input_dim\n        )\n        \n        # Print model stats\n        num_params = sum(p.numel() for p in classifier.model.parameters() if p.requires_grad)\n        print(f\"Model initialized with {num_params:,} trainable parameters\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Training Function\n",
    "\n",
    "Create a function to train on data in batches from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Optimized for p3.2xlarge instances with V100 GPU\ndef train_on_parquet_files(classifier, file_paths, num_epochs=10, patience=5, max_files=None, \n                           max_chunks_per_file=None, validation_split=0.1, num_workers=6, \n                           prefetch_factor=2, pin_memory=True):\n    \"\"\"Train model on data from parquet files in batches, optimized for p3.2xlarge with V100\"\"\"\n    # Limit number of files if specified\n    if max_files is not None:\n        file_paths = file_paths[:max_files]\n    \n    print(f\"Training on {len(file_paths)} parquet files with p3.2xlarge optimizations\")\n    \n    # Set up device\n    device = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")\n    \n    # Initialize CUDA streams for overlapping operations (data transfer + compute)\n    data_stream = torch.cuda.Stream(device=device) if USE_GPU else None\n    \n    # Training metrics storage\n    all_metrics = {\n        'epoch': [],\n        'train_loss': [],\n        'val_loss': [],\n        'train_category_acc': [],\n        'val_category_acc': [],\n        'train_tax_type_acc': [],\n        'val_tax_type_acc': []\n    }\n    \n    # Early stopping variables\n    best_val_loss = float('inf')\n    best_model_state = None\n    patience_counter = 0\n    \n    # Memory management for p3.2xlarge instances\n    if USE_GPU:\n        # Set up gradient accumulation for larger effective batch sizes\n        # Using a conservative 2x accumulation for V100 (16GB)\n        effective_batch_size = BATCH_SIZE * 2  # 2x larger than physical batch size\n        accum_steps = effective_batch_size // BATCH_SIZE\n        print(f\"Using gradient accumulation: {accum_steps} steps for effective batch size of {effective_batch_size}\")\n    else:\n        accum_steps = 1\n    \n    # Training loop over epochs\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training metrics for this epoch\n        epoch_metrics = {\n            'train_loss': [],\n            'val_loss': [],\n            'train_category_acc': [],\n            'val_category_acc': [],\n            'train_tax_type_acc': [],\n            'val_tax_type_acc': []\n        }\n        \n        # Process each file\n        for file_idx, file_path in enumerate(tqdm(file_paths, desc=\"Files\")):\n            # Load and process data in chunks with optimized parallel loading\n            chunk_gen = load_parquet_in_chunks([file_path], \n                                             max_chunks_per_file=max_chunks_per_file,\n                                             num_workers=num_workers)\n            \n            # Set up accumulation variables\n            accum_count = 0\n            \n            for batch_idx, chunk_df in enumerate(tqdm(chunk_gen, desc=f\"Chunks from file {file_idx+1}\", leave=False)):\n                try:\n                    # Use CUDA streams for overlapping operations if GPU is available\n                    if USE_GPU and data_stream is not None:\n                        with torch.cuda.stream(data_stream):\n                            # Prepare data (this will happen on the data stream)\n                            batch_data = classifier.prepare_data(chunk_df)\n                    else:\n                        batch_data = classifier.prepare_data(chunk_df)\n                        \n                    (\n                        transaction_features, seq_features, timestamps,\n                        user_features, is_new_user, transaction_descriptions,\n                        company_features, t0, t1\n                    ) = batch_data\n                    \n                    # Skip empty batches\n                    if transaction_features is None or transaction_features.size(0) == 0:\n                        continue\n                        \n                    # Determine split point for validation\n                    split_idx = int((1 - validation_split) * transaction_features.size(0))\n                    \n                    # Wait for data stream to complete if using GPU\n                    if USE_GPU and data_stream is not None:\n                        torch.cuda.current_stream().wait_stream(data_stream)\n                    \n                    # Training with gradient accumulation for larger effective batch sizes\n                    accum_count += 1\n                    if accum_count == 1:  # First accumulation step\n                        classifier.optimizer.zero_grad()  # Clear gradients\n                    \n                    # Train on training portion\n                    train_metrics = classifier.train_step(\n                        transaction_features[:split_idx], \n                        seq_features[:split_idx], \n                        timestamps[:split_idx],\n                        user_features, \n                        is_new_user[:split_idx] if is_new_user is not None else None, \n                        transaction_descriptions[:split_idx] if transaction_descriptions is not None else None,\n                        company_features[:split_idx] if company_features is not None else None, \n                        t0, t1\n                    )\n                    \n                    # Only step optimizer after accumulation steps\n                    if accum_count >= accum_steps:\n                        accum_count = 0\n                    \n                    # Validate on validation portion\n                    with torch.no_grad():  # No gradients needed for validation\n                        val_metrics = classifier.evaluate(\n                            transaction_features[split_idx:], \n                            seq_features[split_idx:], \n                            timestamps[split_idx:],\n                            user_features, \n                            is_new_user[split_idx:] if is_new_user is not None else None, \n                            transaction_descriptions[split_idx:] if transaction_descriptions is not None else None,\n                            company_features[split_idx:] if company_features is not None else None, \n                            t0, t1\n                        )\n                    \n                    # Accumulate metrics\n                    epoch_metrics['train_loss'].append(train_metrics['loss'])\n                    epoch_metrics['train_category_acc'].append(train_metrics['category_acc'])\n                    epoch_metrics['train_tax_type_acc'].append(train_metrics.get('tax_type_acc', 0))\n                    \n                    epoch_metrics['val_loss'].append(val_metrics['loss'])\n                    epoch_metrics['val_category_acc'].append(val_metrics['category_acc'])\n                    epoch_metrics['val_tax_type_acc'].append(val_metrics.get('tax_type_acc', 0))\n                    \n                    # Clear GPU cache periodically to prevent memory fragmentation on V100\n                    if USE_GPU and batch_idx % 5 == 0:  # More frequent cache clearing for V100\n                        torch.cuda.empty_cache()\n                    \n                except Exception as e:\n                    print(f\"Error processing batch {batch_idx} from file {file_path}: {str(e)}\")\n                    import traceback\n                    traceback.print_exc()\n                    continue\n        \n        # Calculate average metrics for the epoch\n        avg_metrics = {}\n        for key, values in epoch_metrics.items():\n            if values:  # Check if the list is not empty\n                avg_metrics[key] = sum(values) / len(values)\n            else:\n                avg_metrics[key] = 0\n        \n        # Update all metrics history\n        all_metrics['epoch'].append(epoch)\n        for key in ['train_loss', 'val_loss', 'train_category_acc', 'val_category_acc', \n                   'train_tax_type_acc', 'val_tax_type_acc']:\n            all_metrics[key].append(avg_metrics[key])\n        \n        # Print epoch summary\n        print(\n            f\"Epoch {epoch+1}/{num_epochs} | \"\n            f\"Train Loss: {avg_metrics['train_loss']:.4f} | \"\n            f\"Cat Acc: {avg_metrics['train_category_acc']:.4f} | \"\n            f\"Tax Acc: {avg_metrics['train_tax_type_acc']:.4f} | \"\n            f\"Val Loss: {avg_metrics['val_loss']:.4f} | \"\n            f\"Val Cat Acc: {avg_metrics['val_category_acc']:.4f} | \"\n            f\"Val Tax Acc: {avg_metrics['val_tax_type_acc']:.4f}\"\n        )\n        \n        # Check for improvement and early stopping\n        current_val_loss = avg_metrics['val_loss']\n        if current_val_loss < best_val_loss:\n            best_val_loss = current_val_loss\n            best_model_state = classifier.model.state_dict().copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n        \n        # Force memory cleanup after each epoch for V100\n        if USE_GPU:\n            torch.cuda.empty_cache()\n            import gc\n            gc.collect()\n    \n    # Load best model\n    if best_model_state is not None:\n        classifier.model.load_state_dict(best_model_state)\n    \n    return all_metrics\n\n# Add a nullcontext for convenience when not using CUDA streams\nclass nullcontext:\n    def __enter__(self): return None\n    def __exit__(self, *args): pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Train the model on all available parquet files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run training if we have parquet files and model is initialized\nif len(parquet_files) > 0 and hasattr(classifier, 'model') and classifier.model is not None:\n    print(\"Starting training with p3.2xlarge V100 optimizations...\")\n    \n    # Determine optimal number of workers based on vCPUs (p3.2xlarge has 8 vCPUs)\n    import os\n    try:\n        import multiprocessing\n        num_workers = min(6, multiprocessing.cpu_count())  # Use 6 of 8 vCPUs for data loading\n    except:\n        num_workers = 6\n    print(f\"Using {num_workers} worker threads for data loading\")\n    \n    # Train using optimized batch processing for p3.2xlarge instances\n    training_metrics = train_on_parquet_files(\n        classifier=classifier,\n        file_paths=parquet_files,\n        num_epochs=NUM_EPOCHS,\n        patience=PATIENCE,\n        max_files=10,              # Limit to first 10 files for demo\n        max_chunks_per_file=5,     # Limit to first 5 chunks per file for demo\n        validation_split=0.1,\n        num_workers=num_workers,   # Parallel data loading with most vCPUs\n        prefetch_factor=2,         # Prefetch batches for smoother GPU utilization\n        pin_memory=True            # Faster CPU->GPU memory transfer\n    )\n    \n    # Save model for inference\n    model_path = os.path.join(OUTPUT_DIR, MODEL_NAME)\n    \n    # Try to JIT compile the model for faster inference\n    if USE_GPU:\n        try:\n            print(\"Creating TorchScript model for faster inference...\")\n            # Extract the core model for JIT compilation\n            example_input = torch.randn(1, transaction_features.size(1), device=device)\n            example_seq = torch.randn(1, seq_features.size(1), seq_features.size(2), device=device)\n            example_time = torch.randn(1, timestamps.size(1), device=device)\n            try:\n                traced_model = torch.jit.trace(\n                    classifier.model, \n                    example_inputs=(example_input, example_seq, example_input, example_time)\n                )\n                torch.jit.save(traced_model, model_path.replace('.pt', '_jit.pt'))\n                print(f\"TorchScript model saved to {model_path.replace('.pt', '_jit.pt')}\")\n            except Exception as e:\n                print(f\"Could not trace model: {e}, trying script mode\")\n                try:\n                    scripted_model = torch.jit.script(classifier.model)\n                    torch.jit.save(scripted_model, model_path.replace('.pt', '_jit.pt'))\n                    print(f\"TorchScript model saved to {model_path.replace('.pt', '_jit.pt')}\")\n                except Exception as e:\n                    print(f\"Could not create TorchScript model: {e}\")\n        except Exception as e:\n            print(f\"Error creating TorchScript model: {e}\")\n    \n    # Also save in standard format\n    classifier.save_model(model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    # Print memory usage stats if on GPU\n    if USE_GPU:\n        print(\"\\nGPU Memory Usage:\")\n        print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n        print(f\"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n        \n        # Run garbage collection\n        import gc\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(f\"After cleanup - Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\nelse:\n    print(\"Skipping training due to missing data or model initialization error\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Plot the training and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics if available\n",
    "if 'training_metrics' in locals() and training_metrics:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(training_metrics['epoch'], training_metrics['train_loss'], label='Train')\n",
    "    plt.plot(training_metrics['epoch'], training_metrics['val_loss'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot category accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(training_metrics['epoch'], training_metrics['train_category_acc'], label='Train')\n",
    "    plt.plot(training_metrics['epoch'], training_metrics['val_category_acc'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Category Classification Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot tax type accuracy\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(training_metrics['epoch'], training_metrics['train_tax_type_acc'], label='Train')\n",
    "    plt.plot(training_metrics['epoch'], training_metrics['val_tax_type_acc'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Tax Type Classification Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(PLOTS_DIR, 'business_features_training_full.png')\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Training plot saved to {plot_path}\")\n",
    "    \n",
    "    # Display in notebook\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Impact of Business Features\n",
    "\n",
    "Compare model performance with and without business features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def compare_with_without_business_features():\n    \"\"\"Compare model predictions with and without business features - optimized for H100\"\"\"\n    print(\"Running ablation study to analyze business feature impact...\")\n    \n    # Load test data\n    test_files = parquet_files[-2:]  # Use last 2 files as test set\n    test_data = []\n    \n    # Use optimized parallel loading\n    from concurrent.futures import ThreadPoolExecutor\n    \n    def load_test_file(file_path):\n        print(f\"Loading test data from {os.path.basename(file_path)}\")\n        chunks = []\n        chunk_gen = load_parquet_in_chunks([file_path], max_chunks_per_file=2, num_workers=2)  # Use parallel loading\n        for chunk_df in chunk_gen:\n            chunks.append(chunk_df)\n        return pd.concat(chunks) if chunks else None\n    \n    # Load files in parallel\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = list(executor.map(load_test_file, test_files))\n        \n    # Combine results\n    test_data = [df for df in results if df is not None]\n    \n    # Combine test data\n    if not test_data:\n        print(\"No test data available\")\n        return\n    \n    test_df = pd.concat(test_data, ignore_index=True)\n    print(f\"Combined test data shape: {test_df.shape}\")\n    \n    # Process data types to ensure consistency\n    print(\"Preprocessing data types...\")\n    # Convert decimal types to float \n    decimal_cols = [col for col in test_df.columns if str(test_df[col].dtype).startswith('decimal')]\n    for col in decimal_cols:\n        test_df[col] = test_df[col].astype(float)\n    \n    # Ensure categorical columns are strings\n    categorical_cols = [col for col in test_df.columns \n                      if test_df[col].dtype == 'object' \n                      and col not in ['merchant_id', 'category_id', 'user_id', 'txn_id']]\n    for col in categorical_cols:\n        test_df[col] = test_df[col].astype(str)\n    \n    # Set up device\n    device = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")\n    \n    # Put model in evaluation mode and use no_grad for efficiency\n    classifier.model.eval()\n    \n    # Use batching for large datasets to avoid OOM\n    EVAL_BATCH_SIZE = BATCH_SIZE * 2  # Can use larger batch for evaluation\n    MAX_ROWS = 100000  # Limit total rows to evaluate for demo\n    if len(test_df) > MAX_ROWS:\n        print(f\"Limiting evaluation to {MAX_ROWS} rows for demo\")\n        test_df = test_df.sample(MAX_ROWS, random_state=42)\n    \n    # Prepare test data\n    print(\"Preparing data for model...\")\n    batch_data = classifier.prepare_data(test_df)\n    (\n        transaction_features, seq_features, timestamps,\n        user_features, is_new_user, transaction_descriptions,\n        company_features, t0, t1\n    ) = batch_data\n    \n    # Split into batches if necessary\n    num_samples = transaction_features.size(0)\n    num_batches = (num_samples + EVAL_BATCH_SIZE - 1) // EVAL_BATCH_SIZE\n    \n    # Initialize arrays for predictions\n    import numpy as np\n    all_with_company_preds = []\n    all_without_company_preds = []\n    all_true_labels = []\n    \n    # Process in batches\n    print(f\"Evaluating in {num_batches} batches...\")\n    for i in range(num_batches):\n        start_idx = i * EVAL_BATCH_SIZE\n        end_idx = min((i + 1) * EVAL_BATCH_SIZE, num_samples)\n        \n        # Get batch data\n        batch_transaction = transaction_features[start_idx:end_idx]\n        batch_seq = seq_features[start_idx:end_idx]\n        batch_timestamps = timestamps[start_idx:end_idx]\n        batch_is_new_user = is_new_user[start_idx:end_idx] if is_new_user is not None else None\n        batch_descriptions = transaction_descriptions[start_idx:end_idx] if transaction_descriptions is not None else None\n        batch_company = company_features[start_idx:end_idx] if company_features is not None else None\n        \n        # Evaluate with business features\n        with torch.no_grad():\n            # Clear cache before each batch to prevent OOM\n            if USE_GPU:\n                torch.cuda.empty_cache()\n                \n            # Evaluate with company features\n            print(f\"Batch {i+1}/{num_batches}: Evaluating with business features...\")\n            with_company_metrics = classifier.evaluate(\n                batch_transaction, batch_seq, batch_timestamps,\n                user_features, batch_is_new_user, batch_descriptions,\n                batch_company, t0, t1\n            )\n            \n            # Evaluate without business features\n            print(f\"Batch {i+1}/{num_batches}: Evaluating without business features...\")\n            without_company_metrics = classifier.evaluate(\n                batch_transaction, batch_seq, batch_timestamps,\n                user_features, batch_is_new_user, batch_descriptions,\n                None,  # Set company_features to None\n                t0, t1\n            )\n            \n            # Collect predictions\n            all_with_company_preds.append(with_company_metrics['y_category_pred'])\n            all_without_company_preds.append(without_company_metrics['y_category_pred'])\n            all_true_labels.append(with_company_metrics['y_category_true'])\n    \n    # Combine results from all batches\n    y_pred_with = np.concatenate(all_with_company_preds)\n    y_pred_without = np.concatenate(all_without_company_preds)\n    y_true = np.concatenate(all_true_labels)\n    \n    # Calculate overall metrics\n    with_acc = (y_pred_with == y_true).mean()\n    without_acc = (y_pred_without == y_true).mean()\n    \n    # Calculate F1 scores if scikit-learn is available\n    try:\n        from sklearn.metrics import f1_score\n        with_f1 = f1_score(y_true, y_pred_with, average='weighted')\n        without_f1 = f1_score(y_true, y_pred_without, average='weighted')\n        has_f1 = True\n    except:\n        has_f1 = False\n    \n    # Compare results\n    print(\"\\nImpact of Business Features:\")\n    print(f\"Category Accuracy WITH business features: {with_acc:.4f}\")\n    print(f\"Category Accuracy WITHOUT business features: {without_acc:.4f}\")\n    acc_diff = with_acc - without_acc\n    print(f\"Accuracy improvement: {acc_diff:.4f} ({acc_diff*100:.2f}%)\")\n    \n    if has_f1:\n        f1_diff = with_f1 - without_f1\n        print(f\"F1 Score WITH business features: {with_f1:.4f}\")\n        print(f\"F1 Score WITHOUT business features: {without_f1:.4f}\")\n        print(f\"F1 improvement: {f1_diff:.4f} ({f1_diff*100:.2f}%)\")\n    \n    # Count examples where predictions differ\n    diff_count = (y_pred_with != y_pred_without).sum()\n    total_count = len(y_pred_with)\n    print(f\"\\nPredictions differ in {diff_count}/{total_count} examples ({diff_count/total_count*100:.2f}%)\")\n    \n    # Calculate improvement by category\n    correct_with = (y_pred_with == y_true)\n    correct_without = (y_pred_without == y_true)\n    \n    # Cases where business features helped\n    helped = (~correct_without) & correct_with\n    hurt = correct_without & (~correct_with)\n    \n    print(f\"Business features helped in {helped.sum()}/{total_count} cases ({helped.sum()/total_count*100:.2f}%)\")\n    print(f\"Business features hurt in {hurt.sum()}/{total_count} cases ({hurt.sum()/total_count*100:.2f}%)\")\n    \n    # Create result dictionary\n    with_company_metrics = {\n        'category_acc': with_acc,\n        'y_category_pred': y_pred_with,\n        'y_category_true': y_true\n    }\n    \n    without_company_metrics = {\n        'category_acc': without_acc,\n        'y_category_pred': y_pred_without,\n        'y_category_true': y_true\n    }\n    \n    if has_f1:\n        with_company_metrics['category_f1'] = with_f1\n        without_company_metrics['category_f1'] = without_f1\n    \n    return with_company_metrics, without_company_metrics\n\n# Run ablation study if model is available\nif 'classifier' in locals() and hasattr(classifier, 'model') and classifier.model is not None:\n    # Run comparison\n    with_metrics, without_metrics = compare_with_without_business_features()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Predictions with Business Features\n",
    "\n",
    "Show some example predictions with business context"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_business_specific_patterns():\n    \"\"\"Analyze how business features affect predictions for specific industries\"\"\"\n    print(\"Analyzing business-specific prediction patterns...\")\n    \n    # Load a small sample of test data\n    sample_gen = load_parquet_in_chunks([parquet_files[-1]], max_chunks_per_file=1)\n    sample_df = next(sample_gen)\n    \n    # Process data types to ensure consistency\n    # Convert decimal types to float \n    decimal_cols = [col for col in sample_df.columns if str(sample_df[col].dtype).startswith('decimal')]\n    for col in decimal_cols:\n        sample_df[col] = sample_df[col].astype(float)\n    \n    # Ensure categorical columns are strings\n    categorical_cols = [col for col in sample_df.columns \n                      if sample_df[col].dtype == 'object' \n                      and col not in ['merchant_id', 'category_id', 'user_id', 'txn_id']]\n    for col in categorical_cols:\n        sample_df[col] = sample_df[col].astype(str)\n    \n    # Check if industry/company data exists\n    if 'industry_name' not in sample_df.columns:\n        print(\"No industry data found in sample\")\n        return\n    \n    # Get unique industries\n    industries = sample_df['industry_name'].unique()\n    print(f\"Found {len(industries)} unique industries in sample\")\n    \n    # Prepare predictions by industry\n    industry_results = {}\n    \n    for industry in industries[:5]:  # Limit to 5 industries for brevity\n        print(f\"\\nAnalyzing industry: {industry}\")\n        \n        # Filter data by industry\n        industry_df = sample_df[sample_df['industry_name'] == industry].sample(min(50, len(sample_df[sample_df['industry_name'] == industry])))\n        \n        if len(industry_df) == 0:\n            print(f\"No data for industry: {industry}\")\n            continue\n            \n        # Prepare data\n        try:\n            batch_data = classifier.prepare_data(industry_df)\n            (\n                transaction_features, seq_features, timestamps,\n                user_features, is_new_user, transaction_descriptions,\n                company_features, t0, t1\n            ) = batch_data\n            \n            # Get predictions with business features\n            with_company_preds = classifier.predict(\n                transaction_features, seq_features, timestamps,\n                user_features, is_new_user, transaction_descriptions,\n                company_features, t0, t1\n            )\n            \n            # Get predictions without business features\n            without_company_preds = classifier.predict(\n                transaction_features, seq_features, timestamps,\n                user_features, is_new_user, transaction_descriptions,\n                None, t0, t1\n            )\n            \n            # Get ground truth\n            y_true = industry_df['category_id'].values if 'category_id' in industry_df.columns else industry_df['user_category_id'].values\n            \n            # Calculate accuracy\n            acc_with = (with_company_preds == y_true).mean()\n            acc_without = (without_company_preds == y_true).mean()\n            diff_pct = (with_company_preds != without_company_preds).mean() * 100\n            \n            # Store results\n            industry_results[industry] = {\n                'acc_with': acc_with,\n                'acc_without': acc_without,\n                'improvement': acc_with - acc_without,\n                'diff_pct': diff_pct,\n                'sample_size': len(industry_df)\n            }\n            \n            print(f\"Industry: {industry} (n={len(industry_df)})\")\n            print(f\"  Accuracy with business features: {acc_with:.4f}\")\n            print(f\"  Accuracy without business features: {acc_without:.4f}\")\n            print(f\"  Improvement: {acc_with - acc_without:.4f}\")\n            print(f\"  Predictions differ in {diff_pct:.2f}% of cases\")\n            \n            # Show sample transactions where predictions differ\n            diff_indices = np.where(with_company_preds != without_company_preds)[0][:3]  # Get up to 3 examples\n            if len(diff_indices) > 0:\n                print(\"\\n  Sample transactions where business features changed predictions:\")\n                for idx in diff_indices:\n                    orig_idx = industry_df.index[idx]\n                    tx = industry_df.iloc[idx]\n                    print(f\"    Amount: ${tx['amount']:.2f}, Description: {tx['description'][:50]}...\")\n                    print(f\"    With business features: Category {with_company_preds[idx]}\")\n                    print(f\"    Without business features: Category {without_company_preds[idx]}\")\n                    print(f\"    True category: {y_true[idx]}\")\n                    print()\n                    \n        except Exception as e:\n            print(f\"Error analyzing industry {industry}: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            continue\n            \n    # Plot industry comparison\n    if industry_results:\n        industries = list(industry_results.keys())\n        improvements = [industry_results[ind]['improvement'] for ind in industries]\n        diff_pcts = [industry_results[ind]['diff_pct'] for ind in industries]\n        \n        plt.figure(figsize=(12, 6))\n        \n        plt.subplot(1, 2, 1)\n        plt.bar(industries, improvements)\n        plt.xlabel('Industry')\n        plt.ylabel('Accuracy Improvement')\n        plt.title('Business Feature Impact by Industry')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        \n        plt.subplot(1, 2, 2)\n        plt.bar(industries, diff_pcts)\n        plt.xlabel('Industry')\n        plt.ylabel('% Predictions Changed')\n        plt.title('Prediction Changes by Industry')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        \n        # Save plot\n        plt.savefig(os.path.join(PLOTS_DIR, 'industry_impact_analysis.png'))\n        plt.show()\n        \n    return industry_results\n\n# Run industry-specific analysis if model is available\nif 'classifier' in locals() and hasattr(classifier, 'model') and classifier.model is not None:\n    industry_analysis = analyze_business_specific_patterns()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we trained a transaction classification model that incorporates business entity features. The model can now leverage company-specific information like industry, size, and QBO product usage to improve classification accuracy.\n",
    "\n",
    "Key insights:\n",
    "1. Business features have the most significant impact on industry-specific transactions\n",
    "2. The model can handle dimension mismatches gracefully with adaptive projection layers\n",
    "3. The graph-based approach effectively integrates multiple data modalities\n",
    "\n",
    "Next steps:\n",
    "1. Fine-tune the model with additional business-specific data\n",
    "2. Deploy the model for inference in production systems\n",
    "3. Analyze feature importance to better understand which business attributes have the most impact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}